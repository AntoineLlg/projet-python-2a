{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de commentaires YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projet Python de deuxième année à l'ENSAE Paris.\n",
    "\n",
    "Antoine Lelong, Louisa Camadini, Yseult Masson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les commentaires postés sous les vidéos YouTube sont une source d'information, complémentaire avec les \"likes\", pour les créateurs de contenu. Ils permettent au vidéaste de déterminer si son travail a plu ou non à son audience. Cependant, les Youtubeurs n'ont pas nécessairement le temps de tous les lire. Il nous a donc semblé intéressant d'analyser ces commentaires informatiquement, plutôt que de les traiter un à un.\n",
    "\n",
    "Pour ce projet, nous nous sommes penchés sur la chaîne YouTube DirtyBiology, qui fait de la vulgarisation scientifique. Notre choix a été motivé par le fait que les commentaires sous de telles vidéos étaient à la fois nombreux (car la chaîne est relativement connue), constructifs (de par le contenu scientifique) et sans trop de fautes d'ortographes. Cela nous permettait d'avoir des données exploitables pour l'analyse que nous voulions en faire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des modules utiles\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Webscraping, création et nettoyage de la base de données (Antoine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, nous avons utilisé [l'API YouTube Data](https://developers.google.com/youtube/v3), qui nous permet de récupérer les 100 commentaires les plus \"pertinents\" d'une vidéo, sur jusqu'à 20 vidéos différentes, constituant ainsi une base de données de 2000 commentaires, ainsi que quelques données supplémentaires, comme le nombre de likes, le nom d'utilisateur...  \n",
    "Le script scraper.py va chercher ces données et les stocke dans le fichier comments.csv  \n",
    "Les API Google fonctionnent avec des clés d'identification confidentielles, qui n'apparaissent par conséquent pas dans le dépot github. Pour faire fonctionner le script correctement, il faut créer un fichier s'apppelant \".env\" et y placer la ligne suivante :  \n",
    "`APIKEY=\"identifiant_de_la_clé\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python scraper.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Nettoyage de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le script cleaning est prépare les commentaires aux applications des autres parties, en retirant les majuscules, sauts de lignes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python cleaning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"comments.csv\")\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Analyse des commentaires\n",
    "## 2.1 - Analyse exploratoire quantitative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par simplicité, on remplace les identifiants de vidéo par des numéros dans l'ordre de publication (1 pour la plus ancienne, 20 pour la dernière)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dico = {data.videoId.unique()[i] : 20- i for i in range(len(data.videoId.unique()))}\n",
    "data['videoId']=data['videoId'].replace(to_replace=dico)\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certaines données caractérisent la vidéo et non le commentaire, on les récupère dans une autre DataFrame. Elles sont égales sur une vidéo, donc égales à leur max sur ces groupes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_video = data.groupby('videoId')[['viewCount', 'commentCount', 'videoLikeCount','videoDate']].max()\n",
    "info_video['likePerView'] = info_video['videoLikeCount']/info_video['viewCount']\n",
    "info_video['Engagement'] = info_video['commentCount']/info_video['viewCount']\n",
    "info_video.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée une variable correspondant au nombre de mois entre la publication de la vidéo et celle de la dernière vidéo de la base de données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_mois = lambda date : 25 - (int(date[5:7])+12*int(date[3]))\n",
    "info_video['ancienneté'] = info_video.videoDate.apply(nbr_mois)\n",
    "info_video.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "info_video.plot(y='likePerView', ax=axs[0][0], color='r')\n",
    "info_video.plot(y='Engagement', ax=axs[0][0], color='b')\n",
    "axs[0][0].axis([0,20,0,1])\n",
    "axs[0][0].set_title(\"Part d'engagement du public\")\n",
    "\n",
    "info_video.plot(y='likePerView', ax=axs[1][0], color='r')\n",
    "info_video.plot(y='Engagement', ax=axs[1][0], color='b')\n",
    "axs[1][0].set_title(\"Comparaison des proportions de like et commentaires\")\n",
    "\n",
    "info_video.plot(y='Engagement', ax=axs[0][1], color='b')\n",
    "axs[0][1].set_title(\"Variation de l'engagement par commentaires\")\n",
    "\n",
    "\n",
    "slope, intercept, r_value, p_value, std_err = linregress(x=info_video.ancienneté, y=info_video.Engagement)\n",
    "\n",
    "info_video.plot.scatter(x='ancienneté', y='Engagement', ax=axs[1][1])\n",
    "axs[1][1].plot(info_video.ancienneté, slope*info_video.ancienneté + intercept, color='c')\n",
    "axs[1][1].set_title(\"Regression linéaire de l'engagement en commentaires sur l'ancienneté\")\n",
    "plt.text(x=0.5, y=0.006, s=f\"lope = {slope:.2e}\\nR-squared = {r_value**2:.2f} \\nP-value = {p_value:.3f}\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, la part de viewers qui 'likent' et commentent reste stable autour de 9% et 0.4% respectivement, soit une minorité de la totalité des viewers.  \n",
    "On remarque une tendance d'augmentation de la proportion de viewers qui commentent avec l'ancienneté de la vidéo, ce qui suggère que les personnes qui regardent la vidéo tard on plus tendance à commenter. L'interprétation de la régression linéaire donne que pour chaque mois qui passe, cette proportion augmente en moyenne de 0.001 (pour rappelle, elle varie aux alentours de 0.004 sur les observations). La P-valeur à 0.006 montre que le coefficient de la courbe est significatif au seuil 1%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Thèmes majoritairement abordés dans les commentaires (Louisa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports nécessaires pour cette partie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que l'on va s'intéresser ici à la colonne `'textClean'` qui a été nettoyée au préalable. Chaque ligne du dataframe représente un commentaire. L'ensemble de ces lignes, c'est-à-dire tous les commentaires, correspondent donc à notre texte d'étude. Il s'agit tout d'abord de décomposer le texte en unités lexicale, afin de pouvoir analyser les impressions laissées en commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \" \".join(data['textClean'])\n",
    "\n",
    "words = nltk.word_tokenize(content, language='french')\n",
    "#words[200:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La texte a bien été décomposé en 'tokens', mais ce ne sont pas tous des mots : certains peuvent être des signes de ponctuation, des smileys... \n",
    "La méthode `isalpha()` renvoie \"True\" si tous les caractères de la chaîne sont des alphabets, on l'utilise pour ne garder que les 'tokens' qui sont des mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in words if word.isalpha()]\n",
    "#words[200:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, certains mots comme les déterminants, pronoms, sont répétitifs et inutiles à l'analyse du texte. On supprime donc ce qu'on appelle les 'stopwords' :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[200:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création de WordClouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ils permettent de synthétiser les mots et expressions qui reviennent le plus couramment dans les commentaires, et l'on voit qu'ils sont plutôt positifs pour la chaine DirtyBiology !\n",
    "Les images suivantes sont disponibles dans l'onglet [graphes](https://github.com/taucmar/projet-python-2a/tree/main/graphs) de notre dépôt GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a wordcloud\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a ici généré un premier nuage de points classique, composé de 100 mots qui semblent positifs dès le premier coup d'œil : vidéo, travail, merci, incroyable , bravo, intéressant...  \n",
    "On customise ce nuage de mots en l'intégrant au logo YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(\"mask.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word cloud image\n",
    "wc = WordCloud(background_color=\"white\", max_words=90, mask=mask,\n",
    "               stopwords=stop_words, contour_width=3, contour_color='firebrick')\n",
    "\n",
    "# Generate a wordcloud\n",
    "wc.generate(text)\n",
    "\n",
    "# Store to file\n",
    "wc.to_file(\"./graphs/logo_youtube.png\")\n",
    "\n",
    "# Show\n",
    "plt.figure(figsize=[10,8])\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mieux encore, on peut utiliser le logo de DirtyBiology, en retravaillant des images trouvées sur [Internet](https://www.pinterest.fr/pin/148689225175699166/). L'image qui suit a été produite de la même manière."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./graphs/logo_dirty_bio.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Polarisation des commentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 - Calcul de polarités"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le module TextBlob est un module python de NLP utilisé pour l'analyse de sentiments et qui permet, entre autres, de calculer la polarité d'un texte. Celle-ci est définie par un nombre entre -1 et 1, et détermine le degré de satisfaction ou d'insatisfaction qui se dégage du texte. Plus la polaritée est proche de 1, plus le texte dégage un sentiment positif, et vice-versa.\n",
    "\n",
    "Nous avons calculé la polarité de chaque commentaire de deux façons. Nous avons d'une part appliqué directement la fonction du module TextBlob qui nous intéresse au commentaire. D'autre part, nous avons découpé chaque commentaire en phrases, avant d'appliquer à chacune de celles-ci la fonction qui renvoie leur polarisation, puis d'en faire la moyenne sur toutes les phrases du commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blober(text: str) -> float:\n",
    "    '''\n",
    "    Calcule la polarité d'un texte.\n",
    "    \n",
    "    Paramètres : \n",
    "    ----------\n",
    "    text : str\n",
    "        texte dont on veut mesurer la polarité\n",
    "        \n",
    "    Sortie :\n",
    "    ----------\n",
    "    float\n",
    "        polarisation : score entre -1 et 1.\n",
    "        Plus le score est proche de 1, plus le commentaire est positif.\n",
    "        Plus il est proche de -1, plus le commentaire est négatif.\n",
    "    '''\n",
    "    return TextBlob(text, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer()).sentiment[0]\n",
    "\n",
    "blober = np.vectorize(blober)\n",
    "\n",
    "def sentences(comment: str) -> list:\n",
    "    '''\n",
    "    Découpe un texte en phrases.\n",
    "    \n",
    "    Paramètres : \n",
    "    ----------\n",
    "    comment : str\n",
    "        texte dont on veut extraire les phrases\n",
    "        \n",
    "    Sortie :\n",
    "    ----------\n",
    "    list[str]\n",
    "        liste des phrases du texte\n",
    "    '''\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "    return tokenizer.tokenize(comment)\n",
    "\n",
    "def polarisation(comment: str) -> float:\n",
    "    '''\n",
    "    Calcule la polarisation d'un commentaire en calculant d'abord la polarisation de chacune de ses \n",
    "    phrases et en en faisant la moyenne\n",
    "    \n",
    "    Paramètres : \n",
    "    ----------\n",
    "    comment : str\n",
    "        texte dont on veut la polarisation\n",
    "        \n",
    "    Sortie :\n",
    "    ----------\n",
    "    float\n",
    "        moyenne des polarisations des phrases du texte\n",
    "    '''\n",
    "    s = sentences(comment)\n",
    "    return np.mean(blober(s))\n",
    "\n",
    "polarisation = np.vectorize(polarisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ensuite ajouté 2 nouvelles colonnes à la base de données:\n",
    "\n",
    "   - Polarity : polarité de chaque commentaire, calculée sur le commentaire entier directement\n",
    "    \n",
    "   - sentencesPolarity : polarité de chaque commentaire, calculée en prenant la moyenne des polarités sur chaque phrase que comporte le commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Polarity\"] = data.loc[:,['textClean']].apply(blober)\n",
    "data[\"sentencesPolarity\"] = data.loc[:,['textClean']].apply(polarisation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'une colonne de numéros des vidéos (ici ou partie 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DBY'] = data.loc[:,['videoTitle']].apply(np.vectorize(lambda string : string[-7:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Analyse des polarités"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un premier temps, on s'intéresse à la répartition des polarités pour une vidéo, ici la vidéo \"Comment ces champis nous ont façonnés - DBY #76\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['DBY'] == \"DBY #76\", ['Polarity', 'sentencesPolarity']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que pour les deux types de polarité, il y a très peu de valeurs négatives. Cela implique que les commentaires de cette vidéo sont pour la plupart perçus comme positifs.\n",
    "Les écarts type sont de 0.29 et 0.26, ce qui est assez élevé pour des valeurs de polarités allant entre -0.2 (-0.1 pour \"sentencesPolarity\") et 1. Ainsi, les commentaires de cette vidéo dégagent plusieurs degrés de satisfaction, allant de neutre à très satisfait. La moitié d'entre eux sont relativement satisfaits, et ont une polarité se trouvant entre 0.15 et 0.50 (0.12 et 0.45 pour sentencesPolarity).\n",
    "\n",
    "Cela se perçoit bien sur les histogrammes ci-dessous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize = (20, 5))\n",
    "\n",
    "data.loc[data['DBY'] == \"DBY #76\", 'Polarity'].plot.density(ax = axs[0])\n",
    "data.loc[data['DBY'] == \"DBY #76\", 'Polarity'].plot.hist(bins=20, density=True, ax = axs[0])\n",
    "\n",
    "axs[0].set_xlabel(\"Polarité calculée sur le commentaire entier\")\n",
    "axs[0].set_ylabel(\"Nombre de commentaires\")\n",
    "axs[0].set_title(\"Répartition des polarités des commentaires sous la vidéo DBY #76\")\n",
    "\n",
    "data.loc[data['DBY'] == \"DBY #76\", 'sentencesPolarity'].plot.density(ax = axs[1])\n",
    "data.loc[data['DBY'] == \"DBY #76\", 'sentencesPolarity'].plot.hist(bins=20, density=True, ax = axs[1])\n",
    "\n",
    "axs[1].set_xlabel(\"Polarité calculée sur les phrases du commentaire prises une à une\")\n",
    "axs[1].set_ylabel(\"Nombre de commentaires\")\n",
    "axs[1].set_title(\"Répartition des polarités des commentaires sous la vidéo DBY #76\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons ensuite fait la moyenne des polarités sur les 100 commentaires de chaque vidéo, ce qui nous permet de comparer les vidéos entre elles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapol = data.groupby(['DBY']).mean().sort_values('Polarity', ascending = False)\n",
    "datapol.loc[:, ['videoLikeCount', 'Polarity', 'sentencesPolarity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord, nous avons cherché à savoir si les résultats de polarisation restaient cohérents entre les deux méthodes utilisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapol.loc[:,['Polarity','sentencesPolarity']].plot(kind = 'barh', figsize = (11, 7), color = ['lightblue', 'green'], title = 'Comparaison entre polarité sur les commentaires entiers et polarité sur les phrases des commentaires', xlabel = 'Episode de DirtyBiology')\n",
    "plt.legend(labels = ['Polarité : Commentaires entiers', 'Polarité : Phrases prises une à une'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ordre des vidéos change un peu selon la définition que l'on utilise pour la polarité, mais les deux définitions semblent tout de même corrélées (globalement, la polarité prise sur chaque phrase augmente avec celle prise sur le commentaire entier).\n",
    "\n",
    "Dans la suite, on garde la polarité sur le commentaire entier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La polarité est un témoin du succès qu'a eu une vidéo. On cherche à présent à la comparer à un autre marqueur de ce succès, à savoir le ratio de \"likes\" qu'à reçu une vidéo par rapport à son nombre de vues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapol['likeRatio'] = datapol.loc[:,'videoLikeCount'].divide(datapol.loc[:,'viewCount'])\n",
    "datapol.plot(kind = 'scatter', x = 'Polarity', y = 'likeRatio',figsize = (6, 5), marker = 'd', xlabel = 'Polarité', ylabel = 'Ratio de \\\"likes\\\" par rapport au nombre de vues', title = 'Corrélation entre la polarité et le nombre de \\\"likes\\\" d\\'une vidéo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semble ne pas y avoir de corrélation entre ces deux marqueurs. Cela peut être dû au fait que la polarisation ne soit pas très précise, ou que les personnes qui commentent les vidéos n'aient pas le même profil que ceux qui se contentent de \"liker\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Analyse en composantes principales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, nous utilisons la bibliothèque [Sentence Transformer](https://github.com/UKPLab/sentence-transformers) pour transformer les phrases en vecteurs réels. Cela nous permet ensuite de faire une ACP, pour visualiser les différences entre les commentaires, et étudier les plus extrêmes, puisque l'ACP conserve un maximum de variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première étape est de préparer l'encoding :\n",
    "1. On récupère les commentaires\n",
    "2. On isole le texte du reste de la base de données\n",
    "3. Dans la liste font, on fait correspondre à chaque indice de commentaire le numéro de la vidéo associée (pour faire des catégories dans l'ACP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SentenceTransformer(\"xlm-r-bert-base-nli-stsb-mean-tokens\")\n",
    "data = pd.read_csv(\"./comments.csv\")\n",
    "\n",
    "comments = data[['textClean', 'videoId']]\n",
    "\n",
    "text = comments[\"textClean\"].values.tolist()\n",
    "\n",
    "dico_font_video = {}\n",
    "\n",
    "for i, videoid in enumerate(comments.videoId.unique()):\n",
    "    dico_font_video[videoid] = i\n",
    "\n",
    "font = [dico_font_video[videoid] for videoid in comments.videoId]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut ensuite encoder le texte, ce qui les transforme en vecteurs de $\\mathbb{R}^{768}$, qu'on projette après dans $\\mathbb{R}^{2}$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = st.encode(text)\n",
    "pca = PCA(2).fit_transform(embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour une première visualisation, on réunit les commentaires par clusters (dans R^768), et on utilise ces catégories lorsqu'on affiche sur l'ACP dans R^2. On récupère aussi les commentaires les plus proches des centroïdes afin d'avoir des commentaires \"représentants\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_indices = [\n",
    "    int(np.argmin([np.sum((x-centroid)**2) for x in embs]))\n",
    "    for centroid in kmeans.cluster_centers_]\n",
    "\n",
    "commentaires_representants = [comments.textClean[i] for i in center_indices]\n",
    "representants = ''\n",
    "for i, comment in enumerate(commentaires_representants):\n",
    "    representants += f'{i}:{comment} \\n'\n",
    "print(representants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
